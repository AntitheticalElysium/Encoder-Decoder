  ❯❯ /home/antithetical/EPITA/PERSO/Encoder-Decoder : python -m src.train
Loaded 100000 training samples.
Starting training on GPU...
Config: embed_dim=256, hidden_dim=256, batch_size=64
        lr=0.001, clip=5.0, iterations=10000
        vocab_src=14343, vocab_tgt=28692
Iteration 0/10000, Loss: 10.2610, Grad Norm: 0.0000
  -> New best loss. Saving model.
Iteration 100/10000, Loss: 10.2377, Grad Norm: 0.0000
Iteration 200/10000, Loss: 10.2099, Grad Norm: 0.0000
Iteration 300/10000, Loss: 10.1801, Grad Norm: 0.0000
Iteration 400/10000, Loss: 10.1448, Grad Norm: 0.0000
Iteration 500/10000, Loss: 10.1095, Grad Norm: 0.0000
Iteration 600/10000, Loss: 10.0728, Grad Norm: 0.0000
Iteration 700/10000, Loss: 10.0347, Grad Norm: 0.0000
Iteration 800/10000, Loss: 9.9766, Grad Norm: 0.0000
Iteration 900/10000, Loss: 9.9071, Grad Norm: 0.0000
Iteration 1000/10000, Loss: 9.8522, Grad Norm: 0.0000
  -> New best loss. Saving model.
Iteration 1100/10000, Loss: 9.7351, Grad Norm: 0.0000
Iteration 1200/10000, Loss: 9.6263, Grad Norm: 0.0000
Iteration 1300/10000, Loss: 9.4633, Grad Norm: 0.0000
Iteration 1400/10000, Loss: 9.2456, Grad Norm: 0.0000
Iteration 1500/10000, Loss: 8.9812, Grad Norm: 0.0000
Iteration 1600/10000, Loss: 8.6554, Grad Norm: 0.0000
Iteration 1700/10000, Loss: 8.5719, Grad Norm: 0.0000
Iteration 1800/10000, Loss: 8.4132, Grad Norm: 0.0000
Iteration 1900/10000, Loss: 8.4903, Grad Norm: 0.0000
Iteration 2000/10000, Loss: 8.4747, Grad Norm: 0.0000
  -> New best loss. Saving model.
Iteration 2100/10000, Loss: 8.3041, Grad Norm: 0.0000
Iteration 2200/10000, Loss: 8.2201, Grad Norm: 0.0000
Iteration 2300/10000, Loss: 8.3368, Grad Norm: 0.0000
Iteration 2400/10000, Loss: 8.2180, Grad Norm: 0.0000
Iteration 2500/10000, Loss: 8.1301, Grad Norm: 0.0000
Iteration 2600/10000, Loss: 8.2203, Grad Norm: 0.0000
Iteration 2700/10000, Loss: 7.9610, Grad Norm: 0.0000
Iteration 2800/10000, Loss: 8.2564, Grad Norm: 0.0000
Iteration 2900/10000, Loss: 8.1013, Grad Norm: 0.0000
Iteration 3000/10000, Loss: 8.1415, Grad Norm: 0.0000
  -> New best loss. Saving model.

  ❯❯ /home/antithetical/EPITA/PERSO/Encoder-Decoder : python -m src.eval
Loading model and vocabularies
✓ Loaded Seq2Seq model from models/seq2seq.pkl

Starting Translation
==================================================
EN: the cat is blue .
FR: .

EN: i am a student .
FR: .

EN: this is a big house .
FR: .

EN: we are driving a car .
FR: .

EN: she is very smart .
FR: .
